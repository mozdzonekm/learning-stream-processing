---
theme: https://github.com/maaslalani/slides/raw/main/styles/theme.json
---

# âš™ï¸ Project: Stream Processing with Kafka & Flink

This presentation outlines the **learning-stream-processing** project, a pipeline built with Apache Kafka and Apache Flink to process a continuous stream of numerical data.

---

## ğŸ›ï¸ High-Level Architecture

The system features a simple, linear data flow, moving data from a producer through Kafka and a Flink application to a final destination.

* **Producer**: A Rust app that generates a stream of numbers (0-12), intentionally mixed with corrupted data to simulate real-world errors.
* **`raw-topic` (Kafka Topic)**: Ingests the raw, unprocessed data from the producer.
* **Flink Application**: The core processing engine. It consumes from `raw-topic`, filters corrupted data, pairs numbers that sum to 12, and produces the results.
* **`enriched-topic` (Kafka Topic)**: Stores the final, processed pairs of numbers from the Flink application.

---

## ğŸ› ï¸ Docker Components

The entire pipeline is orchestrated using a `docker-compose.yml` file, which defines and links the necessary services.

### ğŸ˜ **Kafka**

* **Service**: `kafka`
* **Purpose**: The central **message broker**. It ingests data from the producer and stores the raw and enriched event streams in their respective topics.
* **Image**: `bitnami/kafka:latest`

### ğŸ–¥ï¸ **Kafka UI**

* **Service**: `kafka-ui`
* **Purpose**: A **web UI** for monitoring and managing the Kafka cluster. It's perfect for inspecting topics and messages in real time.
* **Image**: `ghcr.io/kafbat/kafka-ui:latest`
* **Access**: `http://localhost:8080`

### âš™ï¸ **Flink Cluster**

* **JobManager (`jobmanager`)**: The **master node** that coordinates the application, schedules tasks, and manages state. The Flink Dashboard is available at `http://localhost:8081`.
* **TaskManager (`taskmanager`)**: A **worker node** that executes the actual data processing tasks as instructed by the JobManager.
* **Image**: `flink:2.0.0-scala_2.12-java11` for both.

---

## The Producer

The producer is a Rust application designed to simulate a real-world data stream by sending messages to a Kafka topic.

It generates two message types:

1. **Numeric messages**: A random number within a configured range.
2. **Corrupted messages**: A random alphanumeric string to simulate malformed data.

Key features include configurable message rates, numeric ranges, and error probabilities. It runs indefinitely and supports graceful shutdown.

---

## The Flink Application

The Flink application is the heart of the processing pipeline.

Its main job is to consume events from the `raw-topic` and find pairs of numbers that sum to 12. It also identifies any corrupted (non-numeric) data and routes it to a separate Dead Letter Queue (DLQ) topic for later analysis.

---

## Deserialization / Serialization

Kafka messages are just bytes. Our Flink application needs to understand them, which requires **deserialization** (bytes â†’ objects) before processing and **serialization** (objects â†’ bytes) when writing back.

```java

public class JsonDeserializationSchema<T>
Â  Â  implements DeserializationSchema<DeserializationResult<T>> {

Â  private static final ObjectMapper objectMapper = new ObjectMapper();
Â  private final Class<T> clazz;

Â  public JsonDeserializationSchema(Class<T> clazz) {
Â  Â  this.clazz = clazz;
Â  }

Â  @Override
Â  public DeserializationResult<T> deserialize(byte[] message) {
Â  Â  try {
Â  Â  Â  T item = objectMapper.readValue(message, clazz);
Â  Â  Â  return new DeserializationResult<>(item, message, null);
Â  Â  } catch (Exception e) {
Â  Â  Â  return new DeserializationResult<>(null, message, e);
Â  Â  }
Â  }
}
```

The custom `DeserializationResult` class is crucial here, as it allows us to gracefully handle parsing errors and route corrupted messages to a Dead Letter Queue (DLQ).

---

## Sources and Sinks

In Flink, **Sources** and **Sinks** are abstractions for interacting with external systems.

* A **Source** reads data *into* your Flink application.
* A **Sink** writes data *out* of it.

<!-- end list -->

```java
KafkaSource.<DeserializationResult<Event>>builder()
Â  Â  .setBootstrapServers(bootstrapServers)
Â  Â  .setTopics(rawTopic)
Â  Â  .setGroupId(groupId)
Â  Â  .setValueOnlyDeserializer(new JsonDeserializationSchema<>(Event.class))
Â  Â  .build();

```

---

## Side Outputs

A standard Flink pipeline transforms one stream into another. But what if you need multiple output streams from a single step? That's where **Side Outputs** come in.

We use a side output to split the initial stream: valid events continue for processing, while raw, unparseable messages are sent to the DLQ.

```java

final OutputTag<byte[]> dlqTag = new OutputTag<>("dlq") {};

// ---------------------------------
public class DeserializationResultSplitter<T> extendsÂ 
Â  ProcessFunction<DeserializationResult<T>, T> {

Â  @Override
Â  public void processElement(
Â  Â  Â  DeserializationResult<T> result,
Â  Â  Â  ProcessFunction<DeserializationResult<T>, T>.Context ctx,
Â  Â  Â  Collector<T> out)
Â  Â  Â  throws Exception {
Â  Â  if (result.isSuccess()) out.collect(result.getEvent());
Â  Â  else ctx.output(dlqTag, result.getRaw());
Â  }
}
// ---------------------------------


SingleOutputStreamOperator<Event> events =
Â  Â  env.fromSource(source, WatermarkStrategy.noWatermarks(), "read-events")
Â  Â  Â  Â  .process(new DeserializationResultSplitter<>(dlqTag))
Â  Â  Â  Â  .returns(Event.class);

events.getSideOutput(dlqTag).sinkTo(dlqSink).name("dlq");
```

---

## Stateful Processing

To find pairs, our application must remember numbers it has already seen. This requires **state**.

Since Flink is a distributed system, a simple in-memory `Map` won't work. Flink provides robust, fault-tolerant state mechanisms like `ValueState` for this purpose.

```java
events
Â  Â  .keyBy(event -> event.getPairKey(pairSum))
Â  Â  .process(new PairProcessor(pairSum))
```

The code uses `keyBy` to group numbers that could form a pair (e.g., 3 and 9). The `PairProcessor` then uses `ValueState` to keep a running count for that key, emitting a pair whenever a match is found.

```java
public class PairProcessor extends KeyedProcessFunction<Long, Event, Pair> {
Â  private transient ValueState<Long> pairBalance;

Â  @Override
Â  public void processElement(Event event, Context context, Collector<Pair> collector)
Â  Â  Â  throws Exception {
Â  Â  long num = event.getNum();
Â  Â  Long balance = pairBalance.value();

Â  Â  if (balance == null) {
Â  Â  Â  balance = 0L;
Â  Â  }

Â  Â  if (num < threshold) {
Â  Â  Â  if (balance > 0) collector.collect(new Pair(num, pairSum - num));
Â  Â  Â  balance--;
Â  Â  } else if (num > threshold) {
Â  Â  Â  if (balance < 0) collector.collect(new Pair(pairSum - num, num));
Â  Â  Â  balance++;
Â  Â  } else {
Â  Â  Â  balance++;
Â  Â  Â  if (balance >= 2) {
Â  Â  Â  Â  balance -= 2;
Â  Â  Â  Â  collector.collect(new Pair(num, num));
Â  Â  Â  }
Â  Â  }
Â  Â  pairBalance.update(balance);
Â  }
}
```

---

## ğŸ¬ DEMO

---

## âœ… Conclusion & Key Takeaways

* **Docker simplifies setup**: Getting a Kafka and Flink cluster running locally is straightforward with Docker Compose.
* **Producer performance**: Building a basic data producer is easy, but achieving high throughput requires more complex solutions.
* **Streaming vs. Batch**: Flink's streaming model introduces concepts like serialization and state management that differ from traditional batch processing.
* **Future improvements**: For a production system, adding a Schema Registry and implementing watermarks for event-time processing are critical next steps.
* **AI assistance**: Gemini was a helpful tool for learning and clarifying new concepts during development. ğŸ‘
